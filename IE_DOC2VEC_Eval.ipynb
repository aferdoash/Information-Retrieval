{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Extraction using Doc2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Description\n",
    "\n",
    "Here we have used the Doc2Vec model to find the most similar document/FAQ given a user query.To get going, we'll need to have a set of documents to train our doc2vec model. The bank regulation manual was taken as a reference document for creating a  training data (http://www.bsp.gov.ph/downloads/regulations/morb/morb1.pdf). General FAQs were taken from ICICI Bank (http://www.icicibank.com/Personal-Banking/faq/index.page). Each FAQ is treated as a single document.\n",
    "\n",
    "\n",
    "The steps involved in building the model were as follows :-\n",
    "\n",
    "\n",
    "*\tBuild the tagged corpus based on available banking text files\n",
    "*\tBuild Doc2Vec model based on the tagged corpus\n",
    "*\tFor testing model on the given corpus, infer vector for each document\n",
    "*\tFind the most similar document in the corpus corresponding to this inferred vector\n",
    "*   For test query, infer vector and find its most similar document in the corpus\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Function to Read and Preprocess Text\n",
    "\n",
    "Below, we build the corpus by reading each file,pre-process each document using a simple gensim pre-processing tool (i.e., tokenize text into individual words, remove punctuation, set to lowercase, etc), and return a list of words. Note that, for a given FAQ file, each continuous line constitutes a single document and the length of each line (i.e., document) can vary. Also, to train the model, we'll need to associate a tag/number with each document of the training corpus. In our case, the tag is simply the zero-based line number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/home/afreen/Desktop/BOT')\n",
    "\n",
    "\n",
    "import gensim\n",
    "import glob\n",
    "import io\n",
    "import re\n",
    "\n",
    "corpus=[]\n",
    "all_docs = []\n",
    "list_of_files_1=glob.glob(r'./Banking_General/*.txt')\n",
    "list_of_files_1.sort()\n",
    "i=0\n",
    "for file_name in list_of_files_1:\n",
    "    FI = io.open(file_name,'r',encoding='latin-1')\n",
    "    doc_text = FI.read()\n",
    "    s = gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(doc_text), [i])\n",
    "    i=i+1\n",
    "    corpus.append(s)\n",
    "    all_docs.append(doc_text)\n",
    "    \n",
    "list_of_files_2=glob.glob(r'./Banking_FAQ/*.txt')\n",
    "list_of_files_2.sort()\n",
    "for file_name in list_of_files_2:\n",
    "    FI = io.open(file_name,'r',encoding='latin-1').read()\n",
    "    for line in FI.split('\\n'):\n",
    "        corpus.append(gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(line), [i]))\n",
    "        all_docs.append(line)\n",
    "        i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaggedDocument(words=[u'will', u'the', u'customer', u'be', u'intimated', u'once', u'the', u'mobile', u'number', u'updation', u'is', u'done', u'yes', u'the', u'customer', u'will', u'be', u'intimated', u'by', u'sms', u'on', u'the', u'old', u'and', u'new', u'mobile', u'number', u'once', u'the', u'same', u'is', u'updated', u'in', u'icore'], tags=[158])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[158]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "### Instantiate a Doc2Vec Object\n",
    "\n",
    "Now, we'll instantiate a Doc2Vec model with a vector size with 100 words and iterating over the training corpus 20 times. We set the minimum word count to 2 in order to give higher frequency words more weighting. Model accuracy can be improved by increasing the number of iterations but this generally increases the training time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1737280"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = gensim.models.doc2vec.Doc2Vec(size=100, alpha=0.025, min_alpha=0.025, min_count=1, dm=0, iter=20)\n",
    "model.build_vocab(corpus)\n",
    "model.train(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferring a Vector\n",
    "We can now infer a vector for any piece of text without having to re-train the model by passing a list of words to the model.infer_vector function. This vector can then be compared with other vectors via cosine similarity.\n",
    "We will first test the performance of the model on the corpus itself, that is given an inferred vector of a document, which is the most similar document returned by the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Document (125): «how will get my premium paid receipt the bank will process the receipt»\n",
      "\n",
      "Similar Document (125, 0.9226488471031189): «how will get my premium paid receipt the bank will process the receipt»\n",
      "\n",
      "CPU times: user 980 ms, sys: 0 ns, total: 980 ms\n",
      "Wall time: 1.14 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "top_ranks = []\n",
    "for doc_id in range(len(corpus)):\n",
    "    inferred_vector = model.infer_vector(corpus[doc_id].words)\n",
    "    sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))\n",
    "\n",
    "    top_ranks.append(sims[0])\n",
    "    \n",
    "\n",
    "doc_id = 125 \n",
    "\n",
    "# Compare and print the most/median/least similar documents from the train corpus\n",
    "\n",
    "print('Train Document ({}): «{}»\\n'.format(doc_id, ' '.join(corpus[doc_id].words)))\n",
    "sim_id = top_ranks[doc_id]\n",
    "print('Similar Document {}: «{}»\\n'.format(sim_id, ' '.join(corpus[sim_id[0]].words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model on a user query\n",
    "\n",
    "We will now test the model on a test query by utilizing the methods above, that is preprocessing the query as done for training documents, infer vector for preprocessed query and find the most similar document/FAQ corresponding to the query. A result is returned with document id only if the similarity measure is above a threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process_query(query):\n",
    "    query = gensim.utils.simple_preprocess(query)\n",
    "    inferred_vector= model.infer_vector(query)\n",
    "    sims = model.docvecs.most_similar([inferred_vector], topn=3)\n",
    "    top_match = sims[0]\n",
    "    if top_match[1] > 0.2 :\n",
    "        return top_match[0]\n",
    "    else:\n",
    "        return \"Sorry, I don't have an answer for that.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The FAQ id : 179 \n",
      "\n",
      "How do I find my CVV number ? The CVV number is a code entered on the back of the card. There are 7 digits entered near the signature panel at the back, out of which the last 3 digits are the CVV number. This value is required as a form of authentication for online and IVR transactions.\n"
     ]
    }
   ],
   "source": [
    "query = \"how do i find my cvv number\"\n",
    "result = process_query(query)\n",
    "print \"The FAQ id : %d \\n\" % int(result)\n",
    "print all_docs[int(result)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "We will now evaluate the model based on a Test Corpus by checking for how many FAQs, the model returned the right matching query. For this purpose, we have two Test Corpus: 1) Contains exact matching query 2) Contains queries which are semantically similar. We evaluate the performance of the model separately on these two corpi.\n",
    "\n",
    "### PART 1: EXACTLY MATCHING QUERIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testCorpus = []\n",
    "file_name = r'./TestCorpus/TestFAQs_Direct.txt'\n",
    "testFile = io.open(file_name,'r',encoding='UTF-8').read()\n",
    "for line in testFile.split('\\n'):\n",
    "     testCorpus.append(line)\n",
    "\n",
    "def process_testCorpus(testCorpus):\n",
    "    all_results = []\n",
    "    for each_FAQ in testCorpus:\n",
    "        all_results.append(process_query(each_FAQ))\n",
    "    return all_results\n",
    "\n",
    "all_results = process_testCorpus(testCorpus)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For 21 Exact Questions, accuracy  : 0.666667\n"
     ]
    }
   ],
   "source": [
    "qa= []\n",
    "inds = []\n",
    "\n",
    "for i,query in enumerate(testCorpus):\n",
    "    try:\n",
    "        ans_inds = int(all_results[i])\n",
    "        a = all_docs[ans_inds]\n",
    "    except ValueError:\n",
    "        ans_inds = 9999\n",
    "        a = all_results[i]\n",
    "    qa.append(query + ' MATCHING QUERY :  ' + a)\n",
    "    inds.append(ans_inds)\n",
    "\n",
    "resultFile = io.open(r'./TestCorpus/Results_Direct.txt','w',encoding='UTF-8')\n",
    "\n",
    "for line in qa:\n",
    "    resultFile.write(\"%s\\n\" % line)\n",
    "    \n",
    "answerFile = io.open(r'./TestCorpus/Answers_Direct.txt','r',encoding='latin-1')\n",
    "\n",
    "i=0\n",
    "score = 0.\n",
    "for line in answerFile:\n",
    "     line = (line.strip('\\n')).encode('ascii','ignore')\n",
    "     line = line.split()\n",
    "     line= map(int,line)\n",
    "     if (inds[i] - 95) in line:\n",
    "        score = score+1\n",
    "     i=i+1\n",
    "    \n",
    "accuracy = score/len(inds)\n",
    "print \"For %d Exact Questions, accuracy  : %f\" % (len(inds), accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### PART 2: Semantically similar queries\n",
    "\n",
    "We now test the model against user queries that have semantically similar but not an exactly matching query in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testCorpus = []\n",
    "file_name = r'./TestCorpus/TestFAQs_Indirect.txt'\n",
    "testFile = io.open(file_name,'r',encoding='UTF-8').read()\n",
    "for line in testFile.split('\\n'):\n",
    "     testCorpus.append(line)\n",
    "\n",
    "def process_testCorpus(testCorpus):\n",
    "    all_results = []\n",
    "    for each_FAQ in testCorpus:\n",
    "        all_results.append(process_query(each_FAQ))\n",
    "    return all_results\n",
    "\n",
    "all_results = process_testCorpus(testCorpus)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For 21 Indirect Questions, accuracy  : 0.238095\n"
     ]
    }
   ],
   "source": [
    "qa= []\n",
    "inds = []\n",
    "\n",
    "for i,query in enumerate(testCorpus):\n",
    "    try:\n",
    "        ans_inds = int(all_results[i])\n",
    "        a = all_docs[ans_inds]\n",
    "    except ValueError:\n",
    "        ans_inds = 9999\n",
    "        a = all_results[i]\n",
    "    qa.append(query + ' MATCHING QUERY :  ' + a)\n",
    "    inds.append(ans_inds)\n",
    "\n",
    "resultFile = io.open(r'./TestCorpus/Results_Indirect.txt','w',encoding='UTF-8')\n",
    "\n",
    "for line in qa:\n",
    "    resultFile.write(\"%s\\n\" % line)\n",
    "    \n",
    "answerFile = io.open(r'./TestCorpus/Answers_Indirect.txt','r',encoding='latin-1')\n",
    "\n",
    "i=0\n",
    "score = 0.\n",
    "for line in answerFile:\n",
    "     line = (line.strip('\\n')).encode('ascii','ignore')\n",
    "     line = line.split()\n",
    "     line= map(int,line)\n",
    "     if (inds[i] - 95) in line:\n",
    "        score = score+1\n",
    "     i=i+1\n",
    "    \n",
    "accuracy = score/len(inds)\n",
    "print \"For %d Indirect Questions, accuracy  : %f\" % (len(inds), accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
